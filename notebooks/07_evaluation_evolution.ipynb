{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468a609a",
   "metadata": {},
   "source": [
    "# Scenario 07: Evaluation, Observability, and Prompt Evolution\n",
    "\n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "- Collect and analyze agent performance metrics\n",
    "- Implement accuracy, latency, cost, and quality evaluators\n",
    "- Apply prompt tuning with version tracking\n",
    "- Run A/B tests to compare prompt variants\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Scenario 01 (Simple Agent + MCP)\n",
    "- Familiarity with metrics and observability concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ff8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.common.evaluation import (\n",
    "    MetricsCollector,\n",
    "    MetricType,\n",
    "    ExactMatchEvaluator,\n",
    "    ContainsEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    OpenAICostCalculator,\n",
    "    create_collector,\n",
    "    evaluate_response,\n",
    "    estimate_cost,\n",
    ")\n",
    "from src.common.prompt_tuning import (\n",
    "    PromptTuner,\n",
    "    PromptAnalyzer,\n",
    "    PromptTemplate,\n",
    "    PromptVariable,\n",
    "    create_tuner,\n",
    "    analyze_prompt,\n",
    "    create_template,\n",
    ")\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facce46",
   "metadata": {},
   "source": [
    "## Part 1: Metrics Collection\n",
    "\n",
    "The `MetricsCollector` provides comprehensive metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metrics collector\n",
    "collector = create_collector()\n",
    "\n",
    "print(\"Created MetricsCollector\")\n",
    "print(f\"Default evaluator: {type(collector.evaluator).__name__}\")\n",
    "print(f\"Cost calculator: {type(collector.cost_calculator).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure latency of an operation\n",
    "def simulate_llm_call():\n",
    "    \"\"\"Simulate an LLM API call.\"\"\"\n",
    "    time.sleep(0.1)  # Simulate 100ms latency\n",
    "    return \"Simulated response\"\n",
    "\n",
    "# Use context manager to measure latency\n",
    "with collector.measure_latency(\"llm_call\"):\n",
    "    result = simulate_llm_call()\n",
    "\n",
    "# Check collected metrics\n",
    "latency_metrics = collector.get_metrics(metric_type=MetricType.LATENCY)\n",
    "print(f\"\\nCollected latency metrics: {len(latency_metrics)}\")\n",
    "for m in latency_metrics:\n",
    "    print(f\"  {m.name}: {m.value:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4e393",
   "metadata": {},
   "source": [
    "## Part 2: Accuracy Evaluation\n",
    "\n",
    "Measure how well agent outputs match expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different evaluation methods\n",
    "test_cases = [\n",
    "    (\"Hello world\", \"Hello world\", \"Exact match\"),\n",
    "    (\"Paris\", \"The capital of France is Paris.\", \"Contains\"),\n",
    "    (\"AI is transforming technology\", \"Artificial intelligence is revolutionizing tech\", \"Semantic\"),\n",
    "]\n",
    "\n",
    "print(\"Evaluation Methods Comparison:\\n\")\n",
    "\n",
    "for expected, actual, desc in test_cases:\n",
    "    print(f\"{desc}:\")\n",
    "    print(f\"  Expected: '{expected}'\")\n",
    "    print(f\"  Actual: '{actual}'\")\n",
    "    \n",
    "    # Try different evaluators\n",
    "    exact = evaluate_response(expected, actual, \"exact_match\")\n",
    "    contains = evaluate_response(expected, actual, \"contains\")\n",
    "    semantic = evaluate_response(expected, actual, \"semantic\")\n",
    "    \n",
    "    print(f\"  Exact match: {exact.is_correct} (score: {exact.similarity_score:.2f})\")\n",
    "    print(f\"  Contains: {contains.is_correct} (score: {contains.similarity_score:.2f})\")\n",
    "    print(f\"  Semantic: {semantic.is_correct} (score: {semantic.similarity_score:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record accuracy metrics\n",
    "accuracy = collector.record_accuracy(\n",
    "    expected=\"The answer is 42\",\n",
    "    actual=\"The answer is 42\",\n",
    ")\n",
    "\n",
    "print(f\"Accuracy metric:\")\n",
    "print(f\"  Is correct: {accuracy.is_correct}\")\n",
    "print(f\"  Similarity score: {accuracy.similarity_score}\")\n",
    "print(f\"  Method: {accuracy.evaluation_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eab8ee",
   "metadata": {},
   "source": [
    "## Part 3: Cost Estimation\n",
    "\n",
    "Track and estimate token costs across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate costs for different models\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n",
    "input_tokens = 1000\n",
    "output_tokens = 500\n",
    "\n",
    "print(f\"Cost comparison for {input_tokens} input + {output_tokens} output tokens:\\n\")\n",
    "\n",
    "for model in models:\n",
    "    cost = estimate_cost(input_tokens, output_tokens, model)\n",
    "    print(f\"  {model}: ${cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record cost metrics\n",
    "cost = collector.record_cost(\n",
    "    operation=\"generate_response\",\n",
    "    input_tokens=500,\n",
    "    output_tokens=250,\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "print(f\"Cost metric:\")\n",
    "print(f\"  Operation: {cost.operation}\")\n",
    "print(f\"  Total tokens: {cost.total_tokens}\")\n",
    "print(f\"  Cost USD: ${cost.cost_usd:.6f}\")\n",
    "print(f\"  Model: {cost.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc3066",
   "metadata": {},
   "source": [
    "## Part 4: Quality Scoring\n",
    "\n",
    "Assess response quality across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba740f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record quality scores for different dimensions\n",
    "quality_dimensions = [\n",
    "    (\"relevance\", 0.9, \"Response directly addresses the question\"),\n",
    "    (\"clarity\", 0.85, \"Well-structured and easy to understand\"),\n",
    "    (\"completeness\", 0.75, \"Covers most aspects but missing some detail\"),\n",
    "    (\"accuracy\", 0.95, \"Factually correct information\"),\n",
    "]\n",
    "\n",
    "print(\"Quality Scores:\\n\")\n",
    "\n",
    "for dimension, score, explanation in quality_dimensions:\n",
    "    quality = collector.record_quality(\n",
    "        dimension=dimension,\n",
    "        score=score,\n",
    "        explanation=explanation,\n",
    "    )\n",
    "    print(f\"  {dimension}: {quality.score:.0%} - {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f44f6",
   "metadata": {},
   "source": [
    "## Part 5: Creating Complete Evaluations\n",
    "\n",
    "Combine all metrics into a comprehensive evaluation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete evaluation\n",
    "from src.common.evaluation import LatencyMetric, AccuracyMetric, CostMetric, QualityScore\n",
    "from datetime import datetime\n",
    "\n",
    "evaluation = collector.create_evaluation(\n",
    "    run_id=\"run_001\",\n",
    "    agent_name=\"research_agent\",\n",
    "    latency=LatencyMetric(\n",
    "        operation=\"query\",\n",
    "        duration_ms=150.5,\n",
    "        start_time=datetime.now(),\n",
    "        end_time=datetime.now(),\n",
    "    ),\n",
    "    accuracy=AccuracyMetric(\n",
    "        expected=\"Expected output\",\n",
    "        actual=\"Actual output from agent\",\n",
    "        is_correct=True,\n",
    "        similarity_score=0.85,\n",
    "    ),\n",
    "    cost=CostMetric(\n",
    "        operation=\"query\",\n",
    "        input_tokens=200,\n",
    "        output_tokens=150,\n",
    "        total_tokens=350,\n",
    "        cost_usd=0.00023,\n",
    "        model=\"gpt-4o-mini\",\n",
    "    ),\n",
    "    quality_scores=[\n",
    "        QualityScore(\"relevance\", 0.9, \"Highly relevant\"),\n",
    "        QualityScore(\"clarity\", 0.85, \"Clear response\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Evaluation Result:\")\n",
    "print(f\"  Run ID: {evaluation.run_id}\")\n",
    "print(f\"  Agent: {evaluation.agent_name}\")\n",
    "print(f\"  Overall Quality: {evaluation.overall_quality:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660595dc",
   "metadata": {},
   "source": [
    "## Part 6: Metrics Aggregation\n",
    "\n",
    "Aggregate metrics to understand trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple operations to aggregate\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    # Simulate varying latencies\n",
    "    latency_ms = 100 + random.random() * 100\n",
    "    collector.record_custom(\n",
    "        name=\"api_latency\",\n",
    "        value=latency_ms,\n",
    "        metric_type=MetricType.LATENCY,\n",
    "        tags={\"endpoint\": \"generate\"},\n",
    "    )\n",
    "\n",
    "# Aggregate the results\n",
    "aggregated = collector.aggregate(\"api_latency\")\n",
    "\n",
    "print(\"Aggregated Latency Metrics:\\n\")\n",
    "print(f\"  Count: {aggregated.count}\")\n",
    "print(f\"  Average: {aggregated.avg_value:.2f}ms\")\n",
    "print(f\"  Min: {aggregated.min_value:.2f}ms\")\n",
    "print(f\"  Max: {aggregated.max_value:.2f}ms\")\n",
    "print(f\"  P50: {aggregated.p50_value:.2f}ms\")\n",
    "print(f\"  P95: {aggregated.p95_value:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of all metrics\n",
    "summary = collector.summary()\n",
    "\n",
    "print(\"Metrics Summary:\\n\")\n",
    "print(f\"  Total metrics: {summary['total_metrics']}\")\n",
    "print(f\"  Total evaluations: {summary['total_evaluations']}\")\n",
    "print(f\"  Metric types: {summary['metric_types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f855c6a",
   "metadata": {},
   "source": [
    "## Part 7: Prompt Analysis\n",
    "\n",
    "Analyze prompts for quality and get improvement suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a simple prompt\n",
    "simple_prompt = \"Summarize this text.\"\n",
    "\n",
    "analysis = analyze_prompt(simple_prompt)\n",
    "\n",
    "print(f\"Analysis of: '{simple_prompt}'\\n\")\n",
    "print(f\"  Word count: {analysis.word_count}\")\n",
    "print(f\"  Has role: {analysis.has_role}\")\n",
    "print(f\"  Has examples: {analysis.has_examples}\")\n",
    "print(f\"  Has constraints: {analysis.has_constraints}\")\n",
    "print(f\"  Has output format: {analysis.has_output_format}\")\n",
    "print(f\"\\nSuggestions:\")\n",
    "for s in analysis.suggestions:\n",
    "    print(f\"  - [{s.improvement_type.value}] {s.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41224e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a more complete prompt\n",
    "complete_prompt = \"\"\"\n",
    "You are an expert technical writer.\n",
    "\n",
    "Summarize the following text in exactly 3 bullet points.\n",
    "Focus on the key technical concepts.\n",
    "\n",
    "Important: Do not include opinions, only facts.\n",
    "\n",
    "Example:\n",
    "Input: Python is a programming language known for readability.\n",
    "Output:\n",
    "- Python is a programming language\n",
    "- Known for code readability\n",
    "- Popular for beginners and experts\n",
    "\n",
    "Format: Return as a markdown bullet list.\n",
    "\"\"\"\n",
    "\n",
    "analysis = analyze_prompt(complete_prompt)\n",
    "\n",
    "print(f\"Analysis of complete prompt:\\n\")\n",
    "print(f\"  Word count: {analysis.word_count}\")\n",
    "print(f\"  Has role: {analysis.has_role}\")\n",
    "print(f\"  Has examples: {analysis.has_examples}\")\n",
    "print(f\"  Has constraints: {analysis.has_constraints}\")\n",
    "print(f\"  Has output format: {analysis.has_output_format}\")\n",
    "print(f\"  Complexity: {analysis.complexity_score:.2f}\")\n",
    "print(f\"\\nSuggestions: {len(analysis.suggestions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a94ac1",
   "metadata": {},
   "source": [
    "## Part 8: Prompt Version Tracking\n",
    "\n",
    "Track prompt versions and iterate improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d88806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt tuner\n",
    "tuner = create_tuner()\n",
    "\n",
    "# Register initial prompt version\n",
    "v1 = tuner.create_prompt(\n",
    "    name=\"summarize\",\n",
    "    content=\"Summarize this text.\",\n",
    "    metadata={\"author\": \"team\"},\n",
    ")\n",
    "\n",
    "print(f\"Created prompt:\")\n",
    "print(f\"  Name: summarize\")\n",
    "print(f\"  Version: {v1.version}\")\n",
    "print(f\"  Status: {v1.status.value}\")\n",
    "print(f\"  Hash: {v1.hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the prompt\n",
    "v2 = tuner.iterate(\n",
    "    name=\"summarize\",\n",
    "    new_content=\"\"\"You are a helpful assistant.\n",
    "Summarize the following text concisely.\"\"\",\n",
    "    changes=\"Added role definition\",\n",
    ")\n",
    "\n",
    "v3 = tuner.iterate(\n",
    "    name=\"summarize\",\n",
    "    new_content=\"\"\"You are a helpful assistant.\n",
    "Summarize the following text in 3 bullet points.\n",
    "Focus on key points only.\"\"\",\n",
    "    changes=\"Added structure and constraints\",\n",
    ")\n",
    "\n",
    "print(\"Prompt iterations:\")\n",
    "for v in tuner.registry.list_versions(\"summarize\"):\n",
    "    print(f\"  {v.version}: {v.status.value} - {v.changes or 'Initial version'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get version history\n",
    "history = tuner.get_history(\"summarize\")\n",
    "\n",
    "print(\"Version History:\\n\")\n",
    "for entry in history:\n",
    "    print(f\"  {entry['version']}: {entry['status']} ({entry['hash']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d42150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote a version to active\n",
    "tuner.promote(\"summarize\", \"v1.2\")\n",
    "\n",
    "active = tuner.registry.get_active(\"summarize\")\n",
    "print(f\"Active version: {active.version}\")\n",
    "print(f\"Content: {active.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240fa09",
   "metadata": {},
   "source": [
    "## Part 9: Prompt Templates\n",
    "\n",
    "Create reusable prompt templates with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa56874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template with variables\n",
    "template = create_template(\n",
    "    template=\"\"\"You are a {role}.\n",
    "    \n",
    "{task}\n",
    "\n",
    "Use a {tone} tone.\n",
    "Maximum length: {max_length} words.\"\"\",\n",
    "    variables=[\n",
    "        {\"name\": \"role\", \"description\": \"The AI's role\", \"required\": True},\n",
    "        {\"name\": \"task\", \"description\": \"The task to perform\", \"required\": True},\n",
    "        {\"name\": \"tone\", \"description\": \"Communication tone\", \"default\": \"professional\"},\n",
    "        {\"name\": \"max_length\", \"description\": \"Maximum words\", \"default\": \"200\"},\n",
    "    ],\n",
    "    name=\"task_template\",\n",
    ")\n",
    "\n",
    "print(f\"Template: {template.name}\")\n",
    "print(f\"Variables: {template.get_variable_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the template\n",
    "rendered = template.render(\n",
    "    role=\"data analyst\",\n",
    "    task=\"Analyze the sales data and identify trends.\",\n",
    "    tone=\"casual\",\n",
    ")\n",
    "\n",
    "print(\"Rendered prompt:\\n\")\n",
    "print(rendered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fac0f7",
   "metadata": {},
   "source": [
    "## Part 10: A/B Testing Setup\n",
    "\n",
    "Compare prompt versions to find the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f303c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an A/B test\n",
    "test_config = tuner.compare(\n",
    "    name=\"summarize\",\n",
    "    version_a=\"v1.0\",\n",
    "    version_b=\"v1.2\",\n",
    "    sample_size=100,\n",
    ")\n",
    "\n",
    "print(f\"A/B Test Configuration:\")\n",
    "print(f\"  Test name: {test_config.name}\")\n",
    "print(f\"  Sample size: {test_config.sample_size}\")\n",
    "print(f\"  Metrics: {test_config.metrics}\")\n",
    "print(f\"\\nVariant A: {test_config.variant_a.version}\")\n",
    "print(f\"Variant B: {test_config.variant_b.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb017ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze test results (simulated)\n",
    "result = tuner.ab_runner.analyze(test_config.name)\n",
    "\n",
    "print(f\"A/B Test Results:\")\n",
    "print(f\"\\nVariant A metrics: {result.variant_a_metrics}\")\n",
    "print(f\"Variant B metrics: {result.variant_b_metrics}\")\n",
    "print(f\"\\nWinner: {result.winner.value}\")\n",
    "print(f\"Confidence: {result.confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b4ee5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Exercise: Apply Evaluation-Driven Improvement\n",
    "\n",
    "1. Create a prompt and analyze it\n",
    "2. Apply suggestions to improve it\n",
    "3. Record metrics for both versions\n",
    "4. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# 1. Create initial prompt\n",
    "initial_prompt = \"Write code to sort a list.\"\n",
    "\n",
    "# 2. Analyze and get suggestions\n",
    "initial_analysis = analyze_prompt(initial_prompt)\n",
    "print(f\"Initial prompt analysis:\")\n",
    "print(f\"  Suggestions: {len(initial_analysis.suggestions)}\")\n",
    "for s in initial_analysis.suggestions[:3]:\n",
    "    print(f\"    - {s.description}\")\n",
    "\n",
    "# 3. Create improved version based on suggestions\n",
    "improved_prompt = \"\"\"\n",
    "You are an expert Python developer.\n",
    "\n",
    "Write a function to sort a list of integers in ascending order.\n",
    "\n",
    "Requirements:\n",
    "- Use Python's built-in sorting capabilities\n",
    "- Handle empty lists gracefully\n",
    "- Include type hints\n",
    "\n",
    "Example:\n",
    "Input: [3, 1, 4, 1, 5]\n",
    "Output: [1, 1, 3, 4, 5]\n",
    "\n",
    "Return only the function code, no explanations.\n",
    "\"\"\"\n",
    "\n",
    "improved_analysis = analyze_prompt(improved_prompt)\n",
    "print(f\"\\nImproved prompt analysis:\")\n",
    "print(f\"  Has role: {improved_analysis.has_role}\")\n",
    "print(f\"  Has examples: {improved_analysis.has_examples}\")\n",
    "print(f\"  Has constraints: {improved_analysis.has_constraints}\")\n",
    "print(f\"  Suggestions remaining: {len(improved_analysis.suggestions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e43a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this scenario, you learned:\n",
    "\n",
    "1. **Metrics Collection**: Latency, accuracy, cost, and quality tracking\n",
    "2. **Evaluation Methods**: Exact match, contains, semantic similarity\n",
    "3. **Cost Estimation**: Token-based pricing for different models\n",
    "4. **Prompt Analysis**: Automated suggestions for improvement\n",
    "5. **Version Tracking**: Managing prompt iterations\n",
    "6. **A/B Testing**: Comparing prompt variants\n",
    "7. **Templates**: Reusable prompts with variables\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Measure everything**: You can't improve what you don't measure\n",
    "- **Iterate systematically**: Track changes and their impact\n",
    "- **Test before promoting**: A/B test significant changes\n",
    "- **Analyze prompts**: Use automated analysis to catch issues early\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Integrate evaluation into your CI/CD pipeline\n",
    "- Build dashboards for real-time monitoring\n",
    "- Set up alerts for quality degradation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
